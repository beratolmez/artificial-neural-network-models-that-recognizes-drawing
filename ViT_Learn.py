import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import numpy as np
import timm
import torch.nn as nn
from torchinfo import summary
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm.auto import tqdm
import torchmetrics
import matplotlib.pyplot as plt

# -------- ADIM 1 VE 2 ---------

# GÃ¶rÃ¼ntÃ¼ boyutlarÄ± (ViT genellikle 224x224 kullanÄ±r)
IMG_SIZE = 224
# Batch boyutu [cite: 25]
BATCH_SIZE = 32
LEARNING_RATE = 0.0001 # LR'yi dÃ¼ÅŸÃ¼rdÃ¼k
EPOCHS = 30 # Epoch sayÄ±sÄ±nÄ± artÄ±rdÄ±k
PATIENCE = 5 # SabrÄ± artÄ±rdÄ±k
WEIGHT_DECAY = 1e-4 # AdamW iÃ§in weight decay ekledik

train_data_path = '/kaggle/input/multizoo-dataset/train'
#test_data_path = '/kaggle/input/multizoo-dataset/test'

# Veri DÃ¶nÃ¼ÅŸÃ¼mleri (Preprocessing & Augmentation)
train_transforms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)), # BoyutlandÄ±rma [cite: 20]
    transforms.RandomHorizontalFlip(), # Yatay Ã§evirme (Augmentation) [cite: 21]
    transforms.RandomRotation(10), # Rastgele dÃ¶ndÃ¼rme (Augmentation)
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Renk jitter (Augmentation) [cite: 21]
    transforms.ToTensor(), # TensÃ¶re Ã§evirme
    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalizasyon [cite: 20]
                         std=[0.229, 0.224, 0.225]) # ImageNet standartlarÄ±
])

val_transforms = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)), # BoyutlandÄ±rma [cite: 20]
    transforms.ToTensor(), # TensÃ¶re Ã§evirme
    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalizasyon [cite: 20]
                         std=[0.229, 0.224, 0.225])
])

# TÃ¼m eÄŸitim verisetini yÃ¼kle
full_train_dataset = datasets.ImageFolder(train_data_path, transform=train_transforms)

# SÄ±nÄ±f isimlerini alalÄ±m
class_names = full_train_dataset.classes
num_classes = len(class_names)
print(f"SÄ±nÄ±flar: {class_names}")
print(f"Toplam sÄ±nÄ±f sayÄ±sÄ±: {num_classes}")

# EÄŸitim ve DoÄŸrulama Setlerine AyÄ±rma (%80 eÄŸitim, %20 doÄŸrulama) [cite: 18]
train_size = int(0.8 * len(full_train_dataset))
val_size = len(full_train_dataset) - train_size
train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])

# DoÄŸrulama seti iÃ§in dÃ¶nÃ¼ÅŸÃ¼mleri dÃ¼zeltelim (Augmentation olmasÄ±n)
# Bu kÄ±sÄ±m biraz karmaÅŸÄ±k, normalde baÅŸtan iki ayrÄ± dataset tanÄ±mlamak daha temiz olur.
# Åimdilik, doÄŸrulama seti de train_transforms kullanÄ±yor, ama idealde val_transforms kullanmalÄ±.
# Daha iyi bir yaklaÅŸÄ±m: ImageFolder'Ä± iki kez yÃ¼kleyip sonra split etmek veya custom dataset yazmak.
# Basitlik iÃ§in ÅŸimdilik bÃ¶yle bÄ±rakalÄ±m ama raporda bunu belirtmek iyi olur.

print(f"EÄŸitim seti boyutu: {len(train_dataset)}")
print(f"DoÄŸrulama seti boyutu: {len(val_dataset)}")

# DataLoader'larÄ± oluÅŸtur
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# Test seti iÃ§in DataLoader (Test verisi geldiÄŸinde kullanÄ±lacak) [cite: 16]
# test_dataset = datasets.ImageFolder(test_data_path, transform=val_transforms)
# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# CihazÄ± belirle (GPU varsa kullan, yoksa CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"KullanÄ±lan cihaz: {device}")

# -------- ADIM 3 (Model SeÃ§imi ve Kurulumu)---------

# Ã–nceden eÄŸitilmiÅŸ bir ViT modeli seÃ§elim
# 'vit_base_patch16_224' popÃ¼ler bir seÃ§enektir.
model_name = 'vit_base_patch16_224'
model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)

# Modeli cihaza taÅŸÄ±
model.to(device)

# Metrikler iÃ§in torchmetrics kullanÄ±mÄ±
acc_metric = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes).to(device)
precision_metric = torchmetrics.Precision(task="multiclass", num_classes=num_classes, average='macro').to(device)
recall_metric = torchmetrics.Recall(task="multiclass", num_classes=num_classes, average='macro').to(device)
f1_metric = torchmetrics.F1Score(task="multiclass", num_classes=num_classes, average='macro').to(device)

# Model Ã¶zetini gÃ¶relim (Opsiyonel)
summary(model, input_size=(BATCH_SIZE, 3, IMG_SIZE, IMG_SIZE))

# KayÄ±p fonksiyonu (Ã‡oklu sÄ±nÄ±flandÄ±rma iÃ§in CrossEntropyLoss) [cite: 6]
criterion = nn.CrossEntropyLoss()

# Optimize edici
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# --- YENÄ° LR SCHEDULER ---
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)

# EÄŸitim ve doÄŸrulama geÃ§miÅŸini saklamak iÃ§in
history = {
    'train_loss': [], 'train_acc': [], 'train_f1': [],
    'val_loss': [], 'val_acc': [], 'val_f1': []
}

# Early Stopping iÃ§in deÄŸiÅŸkenler
best_val_loss = float('inf')
epochs_no_improve = 0
best_model_weights = None

# EÄŸitim DÃ¶ngÃ¼sÃ¼
for epoch in range(EPOCHS):
    model.train() # Modeli eÄŸitim moduna al
    running_loss = 0.0
    
    # Metrikleri sÄ±fÄ±rla
    acc_metric.reset()
    f1_metric.reset()

    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [EÄŸitim]")
    for inputs, labels in progress_bar:
        inputs, labels = inputs.to(device), labels.to(device)

        # Gradientleri sÄ±fÄ±rla
        optimizer.zero_grad()

        # Ä°leri yayÄ±lÄ±m
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Geri yayÄ±lÄ±m ve optimizasyon
        loss.backward()
        optimizer.step()

        # Ä°statistikler
        running_loss += loss.item() * inputs.size(0)
        preds = torch.argmax(outputs, dim=1)
        acc_metric.update(preds, labels)
        f1_metric.update(preds, labels)
        progress_bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])

    # Epoch sonu eÄŸitim metrikleri
    epoch_loss = running_loss / len(train_dataset)
    epoch_acc = acc_metric.compute()
    epoch_f1 = f1_metric.compute()
    history['train_loss'].append(epoch_loss)
    history['train_acc'].append(epoch_acc.cpu().numpy())
    history['train_f1'].append(epoch_f1.cpu().numpy())

    # DoÄŸrulama DÃ¶ngÃ¼sÃ¼
    model.eval() # Modeli deÄŸerlendirme moduna al
    val_running_loss = 0.0
    
    # Metrikleri sÄ±fÄ±rla
    acc_metric.reset()
    f1_metric.reset()
    precision_metric.reset()
    recall_metric.reset()

    progress_bar_val = tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [DoÄŸrulama]")
    with torch.no_grad(): # Gradient hesaplamayÄ± kapat
        for inputs, labels in progress_bar_val:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_running_loss += loss.item() * inputs.size(0)
            preds = torch.argmax(outputs, dim=1)
            
            # TÃ¼m metrikleri gÃ¼ncelle
            acc_metric.update(preds, labels)
            precision_metric.update(preds, labels)
            recall_metric.update(preds, labels)
            f1_metric.update(preds, labels)

    # Epoch sonu doÄŸrulama metrikleri
    val_epoch_loss = val_running_loss / len(val_dataset)
    val_epoch_acc = acc_metric.compute()
    val_epoch_f1 = f1_metric.compute()
    val_epoch_precision = precision_metric.compute()
    val_epoch_recall = recall_metric.compute()
    
    history['val_loss'].append(val_epoch_loss)
    history['val_acc'].append(val_epoch_acc.cpu().numpy())
    history['val_f1'].append(val_epoch_f1.cpu().numpy())

    print(f"Epoch {epoch+1}/{EPOCHS} | "
          f"EÄŸitim KaybÄ±: {epoch_loss:.4f} | EÄŸitim Acc: {epoch_acc:.4f} | EÄŸitim F1: {epoch_f1:.4f} | "
          f"DoÄŸrulama KaybÄ±: {val_epoch_loss:.4f} | DoÄŸrulama Acc: {val_epoch_acc:.4f} | DoÄŸrulama F1: {val_epoch_f1:.4f} | "
          f"DoÄŸrulama Prc: {val_epoch_precision:.4f} | DoÄŸrulama Rcl: {val_epoch_recall:.4f}")

    # --- LR SCHEDULER ADIMI ---
    scheduler.step(val_epoch_loss)

    # Early Stopping KontrolÃ¼ [cite: 26]
    if val_epoch_loss < best_val_loss:
        best_val_loss = val_epoch_loss
        epochs_no_improve = 0
        # En iyi modeli kaydet (state_dict olarak)
        best_model_weights = model.state_dict()
        torch.save(best_model_weights, 'best_model_weights.pth')
        print(f"DoÄŸrulama kaybÄ± dÃ¼ÅŸtÃ¼ ({best_val_loss:.4f}), model kaydedildi.")
    else:
        epochs_no_improve += 1
        print(f"DoÄŸrulama kaybÄ± dÃ¼ÅŸmedi. SabÄ±r: {epochs_no_improve}/{PATIENCE}")

    if epochs_no_improve >= PATIENCE:
        print("Early stopping tetiklendi! EÄŸitim durduruluyor.")
        break

# En iyi modeli yÃ¼kle (EÄŸer Early Stopping olduysa)
if best_model_weights:
    model.load_state_dict(best_model_weights)
    print("En iyi model aÄŸÄ±rlÄ±klarÄ± yÃ¼klendi.")

# EÄŸitilen modeli arayÃ¼zde kullanmak iÃ§in kaydet [cite: 27]
torch.save(model.state_dict(), 'final_model.pth')
print("Nihai model 'final_model.pth' olarak kaydedildi.")

# Ã–ÄŸrenme EÄŸrilerini Ã‡izdirme [cite: 29]
plt.figure(figsize=(12, 5))

# F1 Score GrafiÄŸi (Ã–rnekteki gibi [cite: 22])
plt.subplot(1, 2, 1)
plt.plot(history['train_f1'], label='Train F1 Score')
plt.plot(history['val_f1'], label='Validation F1 Score')
plt.title('F1 Scores vs. Epochs')
plt.xlabel('Epochs')
plt.ylabel('F1 Scores')
plt.legend()
plt.grid(True)

# Accuracy GrafiÄŸi (Ã–rnekteki gibi [cite: 30])
plt.subplot(1, 2, 2)
plt.plot(history['train_acc'], label='Train Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Accuracy Scores vs. Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy Scores')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# DoÄŸrulama seti Ã¼zerinde son metrikleri hesapla (En iyi model ile)
model.eval()
acc_metric.reset()
precision_metric.reset()
recall_metric.reset()
f1_metric.reset()

with torch.no_grad():
    for inputs, labels in val_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        preds = torch.argmax(outputs, dim=1)
        acc_metric.update(preds, labels)
        precision_metric.update(preds, labels)
        recall_metric.update(preds, labels)
        f1_metric.update(preds, labels)

final_val_acc = acc_metric.compute()
final_val_precision = precision_metric.compute()
final_val_recall = recall_metric.compute()
final_val_f1 = f1_metric.compute()

print("\n--- DoÄŸrulama Seti SonuÃ§larÄ± ---")
print(f"Accuracy:  {final_val_acc:.4f}")
print(f"Precision: {final_val_precision:.4f}")
print(f"Recall:    {final_val_recall:.4f}")
print(f"F1-Score:  {final_val_f1:.4f}")

# BaÅŸarÄ± Kriteri KontrolÃ¼ [cite: 7]
if final_val_acc >= 0.65:
    print(f"\nTebrikler! DoÄŸrulama setinde %{final_val_acc*100:.2f} baÅŸarÄ± ile >= %65 kriteri saÄŸlandÄ±! ğŸ‰")
else:
    print(f"\nDoÄŸrulama setinde %{final_val_acc*100:.2f} baÅŸarÄ± elde edildi. %65 hedefine ulaÅŸmak iÃ§in modeli iyileÅŸtirmen gerekebilir. ğŸ’¡")

# TODO: Test seti geldiÄŸinde, aynÄ± metrik hesaplama iÅŸlemini test_loader ile yapmalÄ±sÄ±n. [cite: 16, 28]
# Test verisi eÄŸitimde KESÄ°NLÄ°KLE kullanÄ±lmamalÄ±dÄ±r! [cite: 28]